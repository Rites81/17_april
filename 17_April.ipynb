{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1517af",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab07ec5",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "Gradient Boosting Regression is an ensemble technique that builds a strong regression model by sequentially adding weak learners (typically decision trees). Each new learner corrects the errors made by the previous ones by minimizing a loss function (usually the mean squared error for regression). Gradient boosting models are trained iteratively using gradient descent, where the predictions are improved in the direction that reduces the loss the most.\n",
    "\n",
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy\n",
    "Hereâ€™s a simple implementation of Gradient Boosting Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf798b",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class SimpleGradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.init_pred = None\n",
    "\n",
    "    def _fit_tree(self, X, residuals):\n",
    "        # Simple decision stump (tree with max depth 1) for demonstration purposes\n",
    "        thresholds = np.mean(X, axis=0)\n",
    "        best_threshold = None\n",
    "        best_loss = float('inf')\n",
    "        best_split = None\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            left_split = X[:, 0] <= threshold\n",
    "            right_split = X[:, 0] > threshold\n",
    "            loss = mean_squared_error(residuals[left_split], np.mean(residuals[left_split])) + \\\n",
    "                   mean_squared_error(residuals[right_split], np.mean(residuals[right_split]))\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_threshold = threshold\n",
    "                best_split = (left_split, right_split)\n",
    "\n",
    "        return best_threshold, best_split\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.init_pred = np.mean(y)  # Initial prediction (mean of the target values)\n",
    "        residuals = y - self.init_pred  # Compute residuals\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            threshold, (left_split, right_split) = self._fit_tree(X, residuals)\n",
    "            residuals[left_split] -= self.learning_rate * np.mean(residuals[left_split])\n",
    "            residuals[right_split] -= self.learning_rate * np.mean(residuals[right_split])\n",
    "            self.trees.append((threshold, np.mean(residuals[left_split]), np.mean(residuals[right_split])))\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.full(X.shape[0], self.init_pred)\n",
    "        for threshold, left_pred, right_pred in self.trees:\n",
    "            preds[X[:, 0] <= threshold] += self.learning_rate * left_pred\n",
    "            preds[X[:, 0] > threshold] += self.learning_rate * right_pred\n",
    "        return preds\n",
    "\n",
    "# Generate simple data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n",
    "\n",
    "# Fit the model\n",
    "model = SimpleGradientBoostingRegressor(n_estimators=10, learning_rate=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-Squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c50c2f",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth\n",
    "You can experiment with different hyperparameters using grid search or random search to find the optimal configuration. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e71883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best score (MSE): 5775.772062981407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gbr, param_grid, scoring=make_scorer(mean_squared_error), cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score (MSE): {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b2222",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing. In the case of gradient boosting, decision trees with a small depth (often referred to as decision stumps) are typically used as weak learners. These models are weak individually but can be combined to form a strong model through boosting.\n",
    "\n",
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "The intuition behind Gradient Boosting is to sequentially add weak learners to correct the mistakes made by previous learners. At each step, the model learns from the residuals (the difference between the actual values and predictions) and fits a new model to these residuals. Over multiple iterations, the model improves, minimizing the error.\n",
    "\n",
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Gradient Boosting builds an ensemble by training weak learners sequentially. After each learner is trained, it is used to correct the errors made by the previous ones. The contribution of each weak learner is controlled by a learning rate. The ensemble grows by adding weak learners until the desired number of estimators or until the error is sufficiently minimized.\n",
    "\n",
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "Initialize the model: Start with an initial prediction, typically the mean of the target variable for regression.\n",
    "Calculate residuals: Compute the difference between the actual values and the predicted values (residuals).\n",
    "Fit a weak learner: Train a weak learner on the residuals.\n",
    "Update the model: Update the model by adding the predictions of the weak learner to the current model.\n",
    "Iterate: Repeat the process, fitting new learners to the residuals and updating the model until a stopping criterion is reached (e.g., number of estimators, or error threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe60bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
